{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Motivation\n",
    "## 1.1 Introduction\n",
    "Reddit is a large forum visited by millions of people every day, where all content is user currated. To structure this vast website, reddit is seperated into sub forums called subreddits and there exists a subreddit for almost everything imagineable. These subreddits are created by the users and are therefore not a static set of categories like you might see on a news website. A user can subscribe to a set of subreddits to customize the posts it will receive. The user can create posts and comment on posts by other users. This project will analyze the activity pattern of reddit users to find meta communities between subreddits. If you wish to explore one of the subreddits we mention, then you can access any subreddit by going to *www.redddit.com/r/'NameOfSubreddit'*  For a video explination of how reddit works, watch this execellent video by CPG Grey: https://www.youtube.com/watch?v=tlI022aUWQQ\n",
    "\n",
    "This notebook will contain the important code snippits for the project, but not complete blocks of runnable code. \n",
    "\n",
    "## 1.2 The dataset\n",
    "Due to the immense scale of Reddit, some compromises have had to be made in data selection and data processing. \n",
    "The data of this project is based in the month of september 2017 and consists of posts and comments for this time period. It was chosen to limit the data to a single month to make data processing somewhat manageable, as the entire dataset is multiple terabytes. September was chosen because reddit inflates a lot during the summer month, due to the influx of users, with a lot of time on their hands. As reddit is a website many of its users visit many times a week, we asseded that one month of data would be sufficient for the analysis. \n",
    "\n",
    "A comment data point contains the text body, author, score and the name of the subreddit it was submitted to. \n",
    "A post data point contains the title, author, score, whether the post was marked 'not safe for work(NSFW)' and the subreddit it was submitted to. \n",
    "        \n",
    "The dataset of comments is used to analyse the activity of the subreddits. The activity of a subreddit can be quantified by the different users that submit comments to it. If two subreddits have a large intersections of users, a connection is found. \n",
    "The dataset of posts are primarily used in the text analysis. The titles of each post within a community is used to compare content. The original intent was to use the text body of the comment for this, but this approach was chosen to limit the amount of data to process. \n",
    "\n",
    "\n",
    "## 1.3 Goal\n",
    "The goal with this project is to find affiliations across the different subreddits through their active users. The hypothesis is that otherwise unrelated subreddits are affiliated unbeknownst to their users, making way for interesting results about the reddit community. This project will mostly revolve about the communities and how they were found. Further analysis could be done on the outputs of specific meta communities to determine traits in their collective reaction to humor, controversy or external sources used in posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic stats\n",
    "## 2.1 Initial data processing and cleaning\n",
    "The Reddit network created in this project is build from the dataset of comments which is approximately 17GB. Note that more than half of this data is the comment text body. In the inital processing of the data, each subreddit name is replaced with an unique integer and a dictionary is created for later reference. This is done to increase the speed of comparisons and decrease memory ussage. A set of comment authors(users) are assosiated with each subreddit, meaning if you submitted a comment to the given subreddit, you will appear in the set. All combinations of subreddits are iterated and if a combination have intersecting users, an undirected edge will be saved between these. The size of this intersection dictates the wieght of this edge, which will be used in later processing. This process is very intensive as the amount of combinations are equal to number of subreddits squared. This was not manageable with a syncronous python script, so a multi threaded java program was written to compute this. The  program can be found here: https://github.com/ageil/redditgraph/tree/master/Java\n",
    "\n",
    "\n",
    "**Basic statistics for full network and datasets:**\n",
    "\n",
    "Comment dataset(September 2017): \n",
    " * Size: 16.7 GB\n",
    " * Rows(amount of comments): 77.510.000\n",
    " * Total authors(users): 15.108.000\n",
    " * Total subreddits: 70.888\n",
    " \n",
    "Post dataset(September 2017):\n",
    " * Size: 1 GB\n",
    " * Rows(amount of posts): 9800000\n",
    " \n",
    "Full network:\n",
    " * Nodes: 70.888 (Subreddits)\n",
    " * Edges: 225.777.872\n",
    " * Avg. degree: 2999 \n",
    " * Max degree: 47547 (For subreddit 'AskReddit') \n",
    " * Avg. edge weight: 2.6 (Average amount of users in common)\n",
    " \n",
    " \n",
    "Top 10 subreddits by degree:\n",
    "1. AskReddit: 47547\n",
    "2. pics: 38281\n",
    "3. funny: 37812\n",
    "4. todayilearned: 36951\n",
    "5. videos: 35692\n",
    "6. mildlyinteresting: 35615\n",
    "7. worldnews: 35165\n",
    "8. gifs: 34105\n",
    "9. gaming: 32683\n",
    "10. Showerthoughts: 32453\n",
    " \n",
    "The graph below displays the weight destribution with logged axis.\n",
    "<h1><center>Weight distribution of full network</center></h1>\n",
    "![title](https://i.imgur.com/EbwloGi.png)\n",
    " \n",
    "## 2.2 Statistic discussion\n",
    "When you create a Reddit account you will automatically be subscribed to a default set of the most popular subreddits. It is therefore assumed that these subreddits will be among those with the highest degree. This is confirmed by the fact that all subreddits in the top 10 above is default subreddits. \n",
    "\n",
    "\n",
    "It is clear by the distribution of edge weights(users in common) that there are a lot of nodes(subreddits) with few users in common. Some of these edges will have to be removed in order to make further processing meaningful, the upcomming section will explain how the decision of where to cut the network was made. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tools, theory and analysis.\n",
    "## 3.1 Community detection\n",
    "\n",
    "\n",
    "## 3.2 Working with text\n",
    "Because of the scale of this project, it was not possible to go very deep with text analysis as this caused excessive program running times. The text used will consist of all post titles from the network cut. The main focus of the this analysis is the text similarity in a community.  The following hypothesis is proposed:\n",
    "> H1: Text similarity will be higher inside a community when compared to the entire network.\n",
    "\n",
    "This hypthosis will be investigated using term frequency-inverse document frequency. This method reflects on how important a word is for a given document. All titles are first cleaned for stopwords, punctuation, digits, duplicates and finally are all words converted to lower case. Punctuation is removed using a regular expression and text is parsed to the method with utf-8 encoding. The method below is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    stopws = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = nltk.RegexpTokenizer(r'\\w+').tokenize(text) # strip punctuation\n",
    "    tokens = [token for token in tokens if token.lower() not in stopws] # strip stopwords\n",
    "    tokens = [token for token in tokens if not token.isdigit()] # strip digits\n",
    "    tokens = list(set(tokens)) # remove duplicates\n",
    "    clean_text = \" \".join(tokens)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using TF-IDF the documents and corpus will have to be defined. Each subreddit will represent a single document that is the concatination of all post titles associated with that subreddit. The corpus will be collection of all documents associated with the subreddits in network cut. A TF-IDF vectorizer will is used to generate a matrix of the frequency for the different words in each text. The cosine similarity between the different vectors will be found so it is possible to compare two subreddits. The code below uses the *sklearn* module to do these computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tfidf = TfidfVectorizer().fit_transform(textDictFlat.values())\n",
    "cossim = cosine_similarity(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cossim matrix is of size 12083 by 12083 hence representing the amount of subreddits on each axis. To compare similarity within communities, the pairwise similary of subreddits in the community is found. The mean of these pairs represents the text similarity in the community. The mthod below is used to compute this mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def meanCossimForCommunity(i):\n",
    "    subReddits = []\n",
    "    indicies = []\n",
    "    # Find subreddits in group\n",
    "    for key, val in groupDict.items():\n",
    "        if val == i:\n",
    "            subReddits.append(key)\n",
    "    print(str(len(subReddits)) + ' subreddits were found for group ' + str(i))\n",
    "    \n",
    "    # Find indicies of these subreddits corrosponding to cossim matrix\n",
    "    for sub in subReddits:\n",
    "        try:\n",
    "            indicies.append(textDictOrdered.keys().index(sub))\n",
    "        except: \n",
    "            continue\n",
    "    print(str(len(indicies)) + ' indicies were found for group ' + str(i)) \n",
    "    \n",
    "    # Iterate all pairs of subreddits within a group\n",
    "    means = []\n",
    "    for i in indicies:\n",
    "        tempMean = []\n",
    "        for j in indicies:\n",
    "            if not i == j: # Avoid diagonal of matrix where a sub is compared to itself\n",
    "                tempMean.append(cossim[i][j])\n",
    "        means.append(np.mean(tempMean))\n",
    "        \n",
    "    return np.mean(means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this method for the four biggest communites yields the following results:\n",
    "![title](https://i.imgur.com/cO5cCxZ.png)\n",
    "Most noticeable is that community 2 has a lower similarity than the mean of the entire network. Community 2 contains approx. 65% of the total subreddits(nodes), hence catching a substantially broader varity of text. Community 2 also contains most of the default subreddits which makes the text even less reliable, as many different users post there. These results could be improved if the louvain community algorith was better at splitting this very large community. \n",
    "\n",
    "The 3 other communities are all above the total mean, hence containing more similar text. Therefore we fail to reject the hypothesis H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
